{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters for the following models:\n",
    "\n",
    "- Logistic Regression\n",
    "- LightGBM\n",
    "- XGBoost\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import  RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed train and validation data\n",
    "train = pd.read_csv('../data/train_data_processed.csv')\n",
    "valid = pd.read_csv('../data/valid_data_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "\n",
    "X_train = train.drop(columns=['y'])\n",
    "y_train = train['y']\n",
    "\n",
    "X_valid = valid.drop(columns=['y'])\n",
    "y_valid = valid['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28824, 62)\n",
      "(28824,)\n",
      "(6176, 62)\n",
      "(6176,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# fit on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform on train and valid\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_valid_std = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 2.21 s, total: 1min 25s\n",
      "Wall time: 28.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(class_weight='balanced', max_iter=200,\n",
       "                                          random_state=2021),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear']})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "# define the model\n",
    "lr = LogisticRegression(random_state=2021, max_iter=200, class_weight='balanced')\n",
    "cv_lr = GridSearchCV(lr, parameters, cv=5)\n",
    "\n",
    "# run the random search cv on the train set to find the best parameters\n",
    "%time cv_lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: \n",
      " {'C': 100, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters are: \\n', cv_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', max_iter=200,\n",
       "                   random_state=2021, solver='newton-cg')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting with best parameters from the above search\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=2021, \n",
    "    max_iter=200,\n",
    "    class_weight='balanced',\n",
    "    C=0.1,  # best_params_ gives 100 but manual experimentation with this value was better\n",
    "    solver='newton-cg') \n",
    "\n",
    "# fit the model\n",
    "lr_model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ROC-AUC score of Logistic Regression model:  0.8015866888060914\n",
      "Validation ROC-AUC score of Logistic Regression model:  0.7912948885805665\n"
     ]
    }
   ],
   "source": [
    "# predictions  \n",
    "y_predict_train = lr_model.predict_proba(X_train_std)\n",
    "y_predict_valid = lr_model.predict_proba(X_valid_std)\n",
    "\n",
    "# AUC scores\n",
    "train_score_lr = roc_auc_score(y_train, y_predict_train[:,1])\n",
    "valid_score_lr = roc_auc_score(y_valid, y_predict_valid[:,1])\n",
    "print(\"Training ROC-AUC score of Logistic Regression model: \", train_score_lr)\n",
    "print(\"Validation ROC-AUC score of Logistic Regression model: \", valid_score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 45s, sys: 13.2 s, total: 3min 58s\n",
      "Wall time: 35.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=LGBMClassifier(objective='binary',\n",
       "                                            random_state=2021),\n",
       "                   param_distributions={'learning_rate': [0.01, 0.015, 0.025,\n",
       "                                                          0.05, 0.1, 0.3],\n",
       "                                        'max_depth': [-1, 2, 5, 8, 11, 14, 17,\n",
       "                                                      20, 23, 26, 29, 32, 35,\n",
       "                                                      38, 41, 44, 47, 50],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000],\n",
       "                                        'num_leaves': [4, 7, 10, 13, 16, 19, 22,\n",
       "                                                       25, 28, 31, 34, 37, 40,\n",
       "                                                       43, 46, 49]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random search\n",
    "\n",
    "parameters = {\n",
    "    'num_leaves': [x for x in range(4,50, 3)],\n",
    "    'max_depth':[x for x in range(-1,51, 3)],\n",
    "    'learning_rate': [0.01, 0.015, 0.025, 0.05, 0.1, 0.3],\n",
    "    'n_estimators': [x for x in range(100,1001, 100)],\n",
    "}\n",
    "\n",
    "# define the model\n",
    "lgbm = lgb.LGBMClassifier(random_state=2021, objective='binary')\n",
    "# cv_lgbm = GridSearchCV(lgbm, parameters, cv=5)\n",
    "cv_lgbm = RandomizedSearchCV(lgbm, parameters, cv=5)\n",
    "\n",
    "# run the random search cv on the train set to find the best parameters\n",
    "%time cv_lgbm.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: \n",
      " {'num_leaves': 10, 'n_estimators': 900, 'max_depth': 11, 'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters are: \\n', cv_lgbm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.01, max_depth=11, n_estimators=900,\n",
       "               num_leaves=10, objective='binary', random_state=2021)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting with best parameters from the above search\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    random_state=2021, \n",
    "    objective='binary',\n",
    "    num_leaves=10,\n",
    "    max_depth=11,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=900) \n",
    "\n",
    "# fit the model\n",
    "lgb_model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ROC-AUC score of LightGBM model:  0.8366473202026334\n",
      "Validation ROC-AUC score of LightGBM model:  0.8030689360926009\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "y_predict_train = lgb_model.predict_proba(X_train_std)\n",
    "y_predict_valid = lgb_model.predict_proba(X_valid_std)\n",
    "\n",
    "# AUC scores\n",
    "train_score_lgb = roc_auc_score(y_train, y_predict_train[:,1])\n",
    "valid_score_lgb = roc_auc_score(y_valid, y_predict_valid[:,1])\n",
    "print(\"Training ROC-AUC score of LightGBM model: \", train_score_lgb)\n",
    "print(\"Validation ROC-AUC score of LightGBM model: \", valid_score_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachel/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 32s, sys: 1min 7s, total: 49min 40s\n",
      "Wall time: 7min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=0),\n",
       "                   param_distributions={'colsample_bytree': [0.3, 0.4, 0.5, 0.6,\n",
       "                                                             0.7, 0.8],\n",
       "                                        'learning_rate': [0.01, 0.015, 0.025,\n",
       "                                                          0.05, 0.1, 0.3],\n",
       "                                        'max_depth': [3, 4, 6, 8, 10],\n",
       "                                        'min_child_weight': [1, 3, 5, 7],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000],\n",
       "                                        'subsample': [0.3, 0.4, 0.5, 0.6, 0.7,\n",
       "                                                      0.8, 1]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'learning_rate': [0.01, 0.015, 0.025, 0.05, 0.1, 0.3],\n",
    "    'n_estimators': [x for x in range(100,1001, 100)],\n",
    "    'max_depth': [3, 4, 6, 8, 10],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'subsample': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1],\n",
    "    'colsample_bytree': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "}\n",
    "\n",
    "# define the model\n",
    "xgbc = xgb.XGBClassifier(random_state = 2021, verbosity=0)\n",
    "cv_xgb = RandomizedSearchCV(xgbc, parameters, cv=5)\n",
    "\n",
    "# run the random search cv on the train set to find the best parameters\n",
    "%time cv_xgb.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: \n",
      " {'subsample': 0.7, 'n_estimators': 100, 'min_child_weight': 7, 'max_depth': 10, 'learning_rate': 0.025, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters are: \\n', cv_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.025, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=7, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              random_state=2021, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=0.7, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting with best parameters from the above search\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=2021, \n",
    "    learning_rate=0.025,\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_child_weight=7,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.6, \n",
    "    verbosity=0\n",
    ") \n",
    "\n",
    "# fit the model\n",
    "xgb_model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ROC-AUC score of XGBoost model:  0.854777113150471\n",
      "Validation ROC-AUC score of XGBoost model:  0.8074324350731832\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "y_predict_train = xgb_model.predict_proba(X_train_std)\n",
    "y_predict_valid = xgb_model.predict_proba(X_valid_std)\n",
    "\n",
    "# AUC scores\n",
    "train_score_xgb = roc_auc_score(y_train, y_predict_train[:,1])\n",
    "valid_score_xgb = roc_auc_score(y_valid, y_predict_valid[:,1])\n",
    "print(\"Training ROC-AUC score of XGBoost model: \", train_score_xgb)\n",
    "print(\"Validation ROC-AUC score of XGBoost model: \", valid_score_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 28s, sys: 8.37 s, total: 9min 37s\n",
      "Wall time: 9min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=2021),\n",
       "                   param_distributions={'max_depth': [10, 25, 50, 75, 100,\n",
       "                                                      None],\n",
       "                                        'max_features': [15],\n",
       "                                        'n_estimators': [50, 100, 250, 500, 750,\n",
       "                                                         1000]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'max_features': [x for x in range(15,(len(X_train.columns)+1), 50)],\n",
    "    'n_estimators': [50, 100, 250, 500, 750, 1000], \n",
    "    'max_depth': [10, 25, 50, 75, 100] + [None]\n",
    "}\n",
    "\n",
    "# define the model\n",
    "rf = RandomForestClassifier(random_state=2021)\n",
    "cv_rf = RandomizedSearchCV(rf, parameters, cv=5)\n",
    "\n",
    "# run the random search cv on the train set to find the best parameters\n",
    "%time cv_rf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are: \n",
      " {'n_estimators': 500, 'max_features': 15, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters are: \\n', cv_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, max_features=15, n_estimators=500,\n",
       "                       random_state=2021)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting with best parameters from the above search\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    random_state=2021, \n",
    "    n_estimators = 500, \n",
    "    max_features = 15,\n",
    "    max_depth = 10\n",
    ") \n",
    "\n",
    "# fit the model\n",
    "rf_model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ROC-AUC score of Random Forest model:  0.8752111136403355\n",
      "Validation ROC-AUC score of Random Forest model:  0.8051141552511416\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "y_predict_train = rf_model.predict_proba(X_train_std)\n",
    "y_predict_valid = rf_model.predict_proba(X_valid_std)\n",
    "\n",
    "# AUC scores\n",
    "train_score_rf = roc_auc_score(y_train, y_predict_train[:,1])\n",
    "valid_score_rf = roc_auc_score(y_valid, y_predict_valid[:,1])\n",
    "print(\"Training ROC-AUC score of Random Forest model: \", train_score_rf)\n",
    "print(\"Validation ROC-AUC score of Random Forest model: \", valid_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Train ROC-AUC score</th>\n",
       "      <th>Validation ROC-AUC score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.791295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.836647</td>\n",
       "      <td>0.803069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.854777</td>\n",
       "      <td>0.807432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.875211</td>\n",
       "      <td>0.805114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  Train ROC-AUC score  Validation ROC-AUC score\n",
       "0  Logistic Regression             0.801587                  0.791295\n",
       "1             LightGBM             0.836647                  0.803069\n",
       "2              XGBoost             0.854777                  0.807432\n",
       "3        Random Forest             0.875211                  0.805114"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = {\n",
    "    'model':['Logistic Regression', 'LightGBM', 'XGBoost', 'Random Forest'],\n",
    "    'Train ROC-AUC score':[train_score_lr, train_score_lgb, train_score_xgb, train_score_rf],\n",
    "    'Validation ROC-AUC score':[valid_score_lr, valid_score_lgb, valid_score_xgb, valid_score_rf]\n",
    "}\n",
    "\n",
    "pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observations:\n",
    "\n",
    "comparing these tuned models to the initial ones with default parameters in [this notebook](https://github.com/rachelkriggs/rocket/blob/main/notebooks/03_model.ipynb):\n",
    "\n",
    "- The Logistic Regression model overfit a small amount more but overall performed about the same.\n",
    "- The LightGBM model's validation score saw a small decrease; however, this model improved in overfitting less.\n",
    "- The XGBoost model's validation score got better and this model overfit less.\n",
    "- The Random Forest model's validation score increased and the model overfit less; however, this model is still overfitting the most in this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
